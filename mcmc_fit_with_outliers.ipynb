{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line to data using MCMC\n",
    "\n",
    "In this example we will be going over Exercise 6 from [Hogg 2010](http://arxiv.org/abs/1008.4686).  We will be fitting a line to data using a model that rejects outliers using an MCMC sampler.\n",
    "\n",
    "## Packages being used\n",
    "We will make use of the following packages:\n",
    "+ `numpy`: doing math on arrays\n",
    "+ `scipy`: more useful functions here\n",
    "+ `emcee`: this does the heavy lifting for the MCMC code\n",
    "+ `matplotlib`: plot our results\n",
    "+ `astropy`: useful hist function\n",
    "\n",
    "## Relevant documentation\n",
    "Useful documentation:\n",
    "+ `scipy`: http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html\n",
    "+ `emcee`: http://dan.iel.fm/emcee/current/\n",
    "+ `matplotlib`: http://matplotlib.org/1.4.0/api/pyplot_summary.html\n",
    "+ `astropy`: http://docs.astropy.org/en/stable/visualization/histogram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "from matplotlib import pyplot as plt\n",
    "import emcee\n",
    "from astropy.visualization import hist\n",
    "from astropy.table import Table\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "Frist lets read in the data we will be fitting (`.data` returns the raw data array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Table.read('data.csv', format='ascii.csv')\n",
    "x = data['x'].data\n",
    "y = data['y'].data\n",
    "sy = data['sy'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "Lets take a look at our data to see what we are fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.errorbar(x, y, sy, ls='None', mfc='k', ms=2, marker='s', ecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "To use `emcee` we need to define the log(likelihood) function we are trying to maximize.  The likelihood we need for this fit is shown in equation 17 of [Hogg 2010](http://arxiv.org/abs/1008.4686):\n",
    "\n",
    "$$\\mathscr{L} \\propto \\prod_{i=1}^{N}{\\left[\\frac{1-P_{b}}{\\sqrt{2 \\pi \\sigma_{yi}^{2}}} \\exp{\\left(-\\frac{[y_{i} - m x_{i} - b]^2}{2 \\sigma_{yi}^{2}}\\right)} + \\frac{P_{b}}{\\sqrt{2 \\pi [V_{b}+\\sigma_{yi}^{2}]}} \\exp{\\left( -\\frac{[y_{i} - Y_{b}]^2}{2[V_{b}+\\sigma_{yi}^{2}]} \\right)} \\right]}$$\n",
    "\n",
    "where $x_{i}, y_{i}, \\sigma_{yi}$ are the data from the `.csv` file, $m, b$ are the slope and intercept of the line we are fitting, $P_b$ is the probability of a point being an outlier, and $Y_{b}, V_{b}$ are the prameters of the distribution those outliers are draw from.  See section 3 of Hogg's paper for a full derivation.\n",
    "\n",
    "Since we will be using a Bayesian approach to this fitting, we need to define priors for our fit parameters $\\theta = \\left[ m, b, P_b, Y_b, V_b \\right]$.  We will use flat priors in all the parameters except $V_b$ which will be flat on the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    m, b, Pb, Yb, Vb = theta\n",
    "    if (-10 < m < 10) and (-500 < b < 500) and (0 < Pb < 1) and (-100 < Yb < 1000) and (0 < np.log(Vb) < 13):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easy, we will first write a general function of the log(likelihood) of a general Gaussian distribution for the rediusals of the data points from a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnGau(model, y, var):\n",
    "    residual = (y - model)**2\n",
    "    return -0.5 * (np.log(2 * np.pi * var) + residual / var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it easier to write the above equation as a function.  In `emcee` all the data parameters being fit will come in as the frist variable in the log(likelihood) function.  To avoid numerical overflow we will use `scipy`'s `logsumexp` function to define this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, x, y, sy):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        # if the params are outside the prior range return -inf\n",
    "        return -np.inf\n",
    "    m, b, Pb, Yb, Vb = theta\n",
    "    # probability of the data points in the line model\n",
    "    lnpf = lnGau(m * x + b, y, sy**2)\n",
    "    # probability of the data points in the outlier model\n",
    "    lnpb = lnGau(Yb, y, Vb + sy**2)\n",
    "    # combine both probabilities with the propper coefficients and sum them up\n",
    "    lnlike = logsumexp([lnpf, lnpb], b=[[1 - Pb], [Pb]], axis=0).sum()\n",
    "    return lp + lnlike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the MCMC sampler\n",
    "`emcee` uses an ensemble sampling technique where it sends out $N$ walkers to explore the likelihood space and create a sampling from it.  These walkers all need to start in a reasonable part of parameters space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of parameters being fit\n",
    "ndim = 5\n",
    "# number of walkers to use (should be > 2*ndim)\n",
    "nwalkers = 300\n",
    "# good starting point\n",
    "p0 = np.array([1, 240, 0.1, 420, 100])\n",
    "# start each walker in a small ball around this position\n",
    "pos = [p0 + 1e-4 * np.random.randn(5) for i in range(nwalkers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** For the walkers to move around the parameter space the starting positions must all have _finite_ prior probabilities (i.e. `lnprior(pos)` is finitie for all values).\n",
    "\n",
    "### Run the sampler\n",
    "Next we have to set up our sampler and run it.  For this fit we will let it run for 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y, sy))\n",
    "sampler.run_mcmc(pos, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for convergence\n",
    "When that is done running we should check that our walkers have converged on a solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(321)\n",
    "plt.plot(sampler.chain[:, :, 0].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$m$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot(322)\n",
    "plt.plot(sampler.chain[:, :, 1].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$b$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot(323)\n",
    "plt.plot(sampler.chain[:, :, 2].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$P_b$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot(324)\n",
    "plt.plot(sampler.chain[:, :, 3].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$Y_b$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot(325)\n",
    "plt.plot(np.log10(sampler.chain[:, :, 4].T), alpha=0.05, color='k')\n",
    "plt.ylabel(r'$\\log{(V_b)}$')\n",
    "plt.xlabel('step')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the burn-in and find the 1-$\\sigma$ cofidence interval\n",
    "From these plots we can see it took ~400 steps for the MCMC chain to converge, so we will create our final sample from the last half of the chain.  Taking the median of these value will give us our best fit params, and taking the middle 68% will give us our upper and lower confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = sampler.chain[:, 400:, :].reshape((-1, ndim))\n",
    "fits = np.percentile(samples, [16, 50, 84], axis=0)\n",
    "fits_pm = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*fits))\n",
    "print(list(fits_pm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the marginalized posterior probabilities\n",
    "With a sampling from our posterior probability distribution we can visualize the full covariance between $m$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.hexbin(samples[:, 0], samples[:, 1], gridsize=100, cmap='Greys', extent=[2, 2.6, -40, 90])\n",
    "plt.xlabel(r'$m$')\n",
    "plt.ylabel(r'$b$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the fully margalized distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.subplot(121)\n",
    "values, bins, patches = hist(samples[:, 0], bins='knuth', histtype='step', color='k', normed=True)\n",
    "plt.xlabel(r'$m$')\n",
    "plt.subplot(122)\n",
    "values, bins, patches = hist(samples[:, 1], bins='knuth', histtype='step', color='k', normed=True)\n",
    "plt.xlabel(r'$b$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a bonus we can also see what fraction of our data points are outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "# use astropy's hist function so it can pick optimal bin sizes\n",
    "values, bins, patches = hist(samples[:, 2], bins='knuth', histtype='step', color='k', normed=True)\n",
    "plt.xlabel(r'$P_b$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idicates there are ~4 outliers in our sample.\n",
    "\n",
    "### Plot the final fit\n",
    "Now that we have best fit line (along with a represetitive sample) lets place it our data to see how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 300, 500)\n",
    "# pick 40 random parameter sets from the final sample\n",
    "idx = np.random.randint(samples.shape[0], size=40)\n",
    "# plot the data\n",
    "plt.figure(6)\n",
    "plt.errorbar(x, y, sy, ls='None', mfc='k', ms=2, marker='s', ecolor='k')\n",
    "# plot the best fit line\n",
    "plt.plot(X, fits[1][0] * X + fits[1][1], color='k')\n",
    "# plot a sample of best fit lines\n",
    "for i in idx:\n",
    "    f = samples[i]\n",
    "    plt.plot(X, f[0] * X + f[1], color='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this does a good job of finding the best fit line while rejecting outliers.\n",
    "\n",
    "# Exercises\n",
    "1. Repeat the fit but divide all the uncertainty values ($\\sigma_{yi}$) by 2, how does the posterior distribution funciton for $P_b$, $m$, and $b$ change?  Is this what you expected?\n",
    "2. If you wanted to restrict your fits so they all passed thorugh (or very close to) a fixed point $[X_0, Y_0]$ where would this information go?  Modify this code so your fits all pass through the point $[150, 350]$, how does the chnage the posterior for $P_b$.\n",
    "3. How would you model _intrinisic_ scatter within the data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}