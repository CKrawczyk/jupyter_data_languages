{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line to data using MCMC\n",
    "\n",
    "In this example we will be going over Exercise 6 from [Hogg 2010](http://arxiv.org/abs/1008.4686).  We will be fitting a line to data using a model that rejects outliers using an MCMC sampler.\n",
    "\n",
    "## Packages being used\n",
    "+ `numpy`: doing math on arrays\n",
    "+ `scipy`: more useful functions here\n",
    "+ `emcee`: this does the heavy lifting for the MCMC code\n",
    "+ `matplotlib`: plot our results\n",
    "+ `astropy`: useful hist function\n",
    "\n",
    "## Relevant documentation\n",
    "+ `scipy`: http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html\n",
    "+ `emcee`: http://dan.iel.fm/emcee/current/\n",
    "+ `matplotlib`: http://matplotlib.org/1.5.1/api/pyplot_summary.html\n",
    "+ `astropy`: http://docs.astropy.org/en/stable/visualization/histogram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "from matplotlib import pyplot as plt\n",
    "import emcee\n",
    "from astropy.visualization import hist\n",
    "from astropy.table import Table\n",
    "import mpl_style\n",
    "%matplotlib notebook\n",
    "plt.style.use(mpl_style.style1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "First lets read in the data we will be fitting (`.data` returns the raw data array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Table.read('data.csv', format='ascii.csv')\n",
    "x = data['x'].data\n",
    "y = data['y'].data\n",
    "sy = data['sy'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "Lets take a look at our data to see what we are fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.errorbar(x, y, sy, ls='None', mfc='k', ms=5, marker='s', ecolor='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "To use `emcee` we need to define the log(likelihood) function we are trying to maximize.  The likelihood we need for this fit is shown in equation 17 of [Hogg 2010](http://arxiv.org/abs/1008.4686):\n",
    "\n",
    "$$\\mathscr{L} \\propto \\prod_{i=1}^{N}{\\left[\\frac{1-P_{b}}{\\sqrt{2 \\pi \\sigma_{yi}^{2}}} \\exp{\\left(-\\frac{[y_{i} - m x_{i} - b]^2}{2 \\sigma_{yi}^{2}}\\right)} + \\frac{P_{b}}{\\sqrt{2 \\pi [V_{b}+\\sigma_{yi}^{2}]}} \\exp{\\left( -\\frac{[y_{i} - Y_{b}]^2}{2[V_{b}+\\sigma_{yi}^{2}]} \\right)} \\right]}$$\n",
    "\n",
    "where $x_{i}, y_{i}, \\sigma_{yi}$ are the data from the `.csv` file, $m, b$ are the slope and intercept of the line we are fitting, $P_b$ is the probability of a point being an outlier, and $Y_{b}, V_{b}$ are the parameters of the distribution those outliers are draw from.  See section 3 of Hogg's paper for a full derivation.\n",
    "\n",
    "Since we will be using a Bayesian approach to this fitting, we need to define priors for our fit parameters $\\theta = \\left[ m, b, P_b, Y_b, V_b \\right]$.  We will use flat priors in all the parameters except $V_b$ which will be flat on the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    m, b, Pb, Yb, Vb = theta\n",
    "    if (-10 < m < 10) and (-500 < b < 500) and (0 < Pb < 1) and (-100 < Yb < 2000) and (0 < np.log(Vb) < 20):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easy, we will first write a general function of the log(likelihood) of a general Gaussian distribution for the residuals of the data points from a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnGau(model, y, var):\n",
    "    residual = (y - model)**2\n",
    "    return -0.5 * (np.log(2 * np.pi * var) + residual / var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it easier to write the above equation as a function.  In `emcee` all the data parameters being fit will come in as the first variable in the log(likelihood) function.  To avoid numerical overflow we will use `scipy`'s `logsumexp` function to define this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, x, y, sy):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        # if the params are outside the prior range return -inf\n",
    "        return -np.inf\n",
    "    m, b, Pb, Yb, Vb = theta\n",
    "    # probability of the data points in the line model\n",
    "    lnpf = lnGau(m * x + b, y, sy**2)\n",
    "    # probability of the data points in the outlier model\n",
    "    lnpb = lnGau(Yb, y, Vb + sy**2)\n",
    "    # combine both probabilities with the propper coefficients and sum them up\n",
    "    lnlike = logsumexp([lnpf, lnpb], b=[[1 - Pb], [Pb]], axis=0).sum()\n",
    "    return lp + lnlike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the MCMC sampler\n",
    "`emcee` uses an ensemble sampling technique where it sends out $N$ walkers to explore the likelihood space and create a sampling from it.  These walkers all need to start in a reasonable part of parameters space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of parameters being fit\n",
    "ndim = 5\n",
    "# number of walkers to use (should be > 2*ndim)\n",
    "nwalkers = 100\n",
    "# good starting point\n",
    "p0 = np.array([1, 240, 0.1, 420, 100])\n",
    "# start each walker in a small ball around this position\n",
    "pos = [p0 + 1e-4 * np.random.randn(5) for i in range(nwalkers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** For the walkers to move around the parameter space the starting positions must all have _finite_ prior probabilities (i.e. `lnprior(pos)` is finite for all values).\n",
    "\n",
    "### Run the sampler\n",
    "Next we have to set up our sampler and run it.  For this fit we will let it run for 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y, sy))\n",
    "sampler.run_mcmc(pos, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for convergence\n",
    "When that is done running we should check that our walkers have converged on a solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(8, 8))\n",
    "plt.subplot2grid((3, 2), (0, 0))\n",
    "plt.plot(sampler.chain[:, :, 0].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$m$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot2grid((3, 2), (0, 1))\n",
    "plt.plot(sampler.chain[:, :, 1].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$b$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot2grid((3, 2), (1, 0))\n",
    "plt.plot(sampler.chain[:, :, 2].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$P_b$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot2grid((3, 2), (1, 1))\n",
    "plt.plot(sampler.chain[:, :, 3].T, alpha=0.05, color='k')\n",
    "plt.ylabel(r'$Y_b$')\n",
    "plt.xlabel('step')\n",
    "plt.subplot2grid((3, 2), (2, 0))\n",
    "plt.plot(np.log(sampler.chain[:, :, 4].T), alpha=0.05, color='k')\n",
    "plt.ylabel(r'$\\ln{(V_b)}$')\n",
    "plt.xlabel('step')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the burn-in and find the 1-$\\sigma$ cofidence interval\n",
    "From these plots we can see it took ~400 steps for the MCMC chain to converge, so we will create our final sample from the last half of the chain.  Taking the median of these value will give us our best fit params, and taking the middle 68% will give us our upper and lower confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampler.chain has shape [nwalkers, nsteps, ndim]\n",
    "samples = sampler.chain[:, 400:, :].reshape((-1, ndim))\n",
    "fits = np.percentile(samples, [16, 50, 84], axis=0)\n",
    "fits_pm = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*fits))\n",
    "print(list(fits_pm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the marginalized posterior probabilities\n",
    "With a sampling from our posterior probability distribution we can visualize the full covariance between $m$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.hexbin(samples[:, 0], samples[:, 1], gridsize=100, mincnt=1, cmap='Greys', extent=[2, 2.6, -40, 90])\n",
    "plt.xlabel(r'$m$')\n",
    "plt.ylabel(r'$b$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the fully marginalized distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(8, 5))\n",
    "plt.subplot2grid((1, 2), (0, 0))\n",
    "values, bins, patches = hist(samples[:, 0], bins='knuth', histtype='step', color='k', normed=True, lw=1.5)\n",
    "plt.xlabel(r'$m$')\n",
    "plt.ylabel(r'$P(m)$')\n",
    "plt.subplot2grid((1, 2), (0, 1))\n",
    "values, bins, patches = hist(samples[:, 1], bins='knuth', histtype='step', color='k', normed=True, lw=1.5)\n",
    "plt.xlabel(r'$b$')\n",
    "plt.ylabel(r'$P(b)$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a bonus we can also see what fraction of our data points are outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(5, figsize=(6, 5))\n",
    "# use astropy's hist function so it can pick optimal bin sizes\n",
    "values, bins, patches = hist(samples[:, 2], bins='knuth', histtype='step', color='k', normed=True, lw=1.5)\n",
    "plt.xlabel(r'$P_b$')\n",
    "plt.ylabel(r'$P(P_b)$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates there are ~4 outliers in our sample.\n",
    "\n",
    "### Plot the final fit\n",
    "Now that we have best fit line (along with a representative sample) lets place it our data to see how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 300, 500)\n",
    "# pick 40 random parameter sets from the final sample\n",
    "idx = np.random.randint(samples.shape[0], size=40)\n",
    "# plot the data\n",
    "plt.figure(6)\n",
    "plt.errorbar(x, y, sy, ls='None', mfc='k', ms=5, marker='s', ecolor='k')\n",
    "# plot the best fit line\n",
    "plt.plot(X, fits[1][0] * X + fits[1][1], color='k')\n",
    "# plot a sample of best fit lines\n",
    "for i in idx:\n",
    "    f = samples[i]\n",
    "    plt.plot(X, f[0] * X + f[1], color='k', alpha=0.1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this does a good job of finding the best fit line while rejecting outliers.\n",
    "\n",
    "Working through a bit of math, we can also determine what points are the outliers.  The function below takes in a sample from the posterior probability distribution and calculates the likelihoods for each data point coming from the foreground and background distributions (with the priors).  If it has a higher likelihood of coming from the background it will return `True`, otherwise `False`.  Evaluating this function at each sample position, and taking the median over all samples gives us a mask we can use to select the outliers and plot them as a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_outlier(theta, x, y, sy):\n",
    "    m, b, Pb, Yb, Vb = theta\n",
    "    lp = lnprior(theta)\n",
    "    # probability of the data points in the line model (with the prior)\n",
    "    lnpf = lnGau(m * x + b, y, sy**2) + np.log(1 - Pb) + lp\n",
    "    # probability of the data points in the outlier model (with the prior)\n",
    "    lnpb = lnGau(Yb, y, Vb + sy**2) + np.log(Pb) + lp\n",
    "    return (lnpb > lnpf)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(7)\n",
    "plt.errorbar(x, y, sy, ls='None', mfc='k', ms=5, marker='s', ecolor='k')\n",
    "# plot the best fit line\n",
    "plt.plot(X, fits[1][0] * X + fits[1][1], color='k')\n",
    "# plot a sample of best fit lines\n",
    "for i in idx:\n",
    "    f = samples[i]\n",
    "    plt.plot(X, f[0] * X + f[1], color='k', alpha=0.1)\n",
    "# get the outliers\n",
    "q_sample = np.array([check_outlier(sample, x, y, sy) for sample in samples])\n",
    "q_mask = np.median(q_sample, axis=0).astype(bool)\n",
    "plt.plot(x[q_mask], y[q_mask], 'o', mfc='none', mec='r', ms=20, mew=1.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Using flat priors for $m$ and $\\ln(V_b)$ are not uninformative priors due to various symmetry reasons (see http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/). To correct this our code should be using $P(\\theta) \\propto \\frac{1}{V_b} \\left( 1 + m^2 \\right)^{-3/2}$.  Adjust the above code to use this new prior, how does using this prior change the results?\n",
    "2. Repeat the fit but divide all the uncertainty values ($\\sigma_{yi}$) by 2, how does the posterior distribution function for $P_b$, $m$, and $b$ change?  Is this what you expected?\n",
    "3. If you wanted to restrict your fits so they all passed through (or very close to) a fixed point $[X_0, Y_0]$ where would this information go?  Modify this code so your fits all pass through the point $[150, 350]$, how does the change the posterior for $P_b$.\n",
    "4. How would you model _intrinsic_ scatter within the data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}