{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line to data using MCMC\n",
    "\n",
    "In this example we will be going over Exercise 6 from [Hogg 2010](http://arxiv.org/abs/1008.4686).  We will be fitting a line to data using a model that rejects outliers using an MCMC sampler.\n",
    "\n",
    "## Packages being used\n",
    "+ `numpy`: doing math on arrays\n",
    "+ `pymc3`: this does the heavy lifting for the MCMC code\n",
    "+ `matplotlib`: plot our results\n",
    "+ `seaborn`: useful plotting functions\n",
    "+ `python-graphviz`: plotting pymc models as graphs\n",
    "+ `pandas`: read in data table\n",
    "\n",
    "## Relevant documentation\n",
    "+ introduction to probabilistic programming: https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Prologue/Prologue.ipynb\n",
    "+ `pymc3`: https://docs.pymc.io/notebooks/GLM-robust-with-outlier-detection.html#5.-Linear-Model-with-Custom-Likelihood-to-Distinguish-Outliers:-Hogg-Method\n",
    "+ `matplotlib`: https://matplotlib.org/api/pyplot_summary.html\n",
    "+ how to pick priors: https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import pandas\n",
    "import seaborn\n",
    "import mpl_style\n",
    "%matplotlib inline\n",
    "plt.style.use(mpl_style.style1)\n",
    "seaborn.axes_style(mpl_style.style1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "First lets read in the data we will be fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('data.csv')\n",
    "\n",
    "x_mean = data.x.mean()\n",
    "x_std = data.x.std()\n",
    "y_mean = data.y.mean()\n",
    "y_std = data.y.std()\n",
    "\n",
    "# center and scale data\n",
    "x_center = (data.x - x_mean) / x_std\n",
    "y_center = (data.y - y_mean) / y_std\n",
    "sy_center = data.sy / y_std\n",
    "sx_center = data.sx / x_std\n",
    "\n",
    "# data order\n",
    "idx = data.x.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does centering the data do?\n",
    "We will be fitting a line to our data with a slope and an intercept.  In the original data space these two value are highly correlated, a small change in slope will result in a large change in the intercept and vice-versa.  By centering and scaling the data we are ensuring the intercept is close to 0 and the `x` and `y` values are about the same size.  This reduces the correlation between the two fit parameters (e.g. you can adjust the slope and keep the intercept the same).\n",
    "\n",
    "### Plot the data\n",
    "Lets take a look at our data to see what we are fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(12, 8))\n",
    "plt.errorbar(\n",
    "    data.x,\n",
    "    data.y,\n",
    "    data.sy,\n",
    "    ls='None',\n",
    "    mfc='k',\n",
    "    mec='k',\n",
    "    ms=5,\n",
    "    marker='s',\n",
    "    ecolor='k'\n",
    ")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700)\n",
    "\n",
    "plt.figure(12, figsize=(12, 8))\n",
    "plt.errorbar(\n",
    "    x_center,\n",
    "    y_center,\n",
    "    sy_center,\n",
    "    ls='None',\n",
    "    mfc='k',\n",
    "    mec='k',\n",
    "    ms=5,\n",
    "    marker='s',\n",
    "    ecolor='k'\n",
    ")\n",
    "plt.xlabel('Centered x')\n",
    "plt.ylabel('Centered y')\n",
    "plt.gca().set_aspect(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Model\n",
    "As a starting point we will first build a least squares linear regression model and see how that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mdl_ols:\n",
    "    # Define weakly informative Normal priros (Ridge regression)\n",
    "    b0 = pm.Normal('intercept', mu=0, sd=100)\n",
    "    b1 = pm.Normal('slope', mu=0, sd=100)\n",
    "    # Define y_est as a Deterministic variable so we can plot it later\n",
    "    y_est = pm.Deterministic(r'$y_{est}$', b0 + b1 * x_center)\n",
    "    likelihood = pm.Normal('likelihood', mu=y_est, sd=sy_center, observed=y_center)\n",
    "\n",
    "display(mdl_ols)\n",
    "display(pm.model_to_graphviz(mdl_ols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the likelihood of the fit given the data is defined as a Normal distribution with scatter defined by observed y-errors.  The priors chosen for the slope and intercept are weakly informative Normal priors. The use of these particular priors is called Ridge regression.\n",
    "\n",
    "### Run MCMC\n",
    "Now we can run the MCMC sampler to find the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_ols:\n",
    "    traces_ols = pm.sample(3000, tune=2000, chains=3, cores=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default sampler is NUTS (No U-True Sampler), a powerful sampler that uses the likelihood's gradient information to propose new MC steps. \n",
    "\n",
    "### Check for convergence\n",
    "Lets look at the results and see if the MCMC converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_ols:\n",
    "    display(pm.summary(traces_ols, var_names=['intercept', 'slope']))\n",
    "    pm.traceplot(traces_ols, var_names=['intercept', 'slope']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good, the `Rhat` values are all close to 1, and the traces have all mixed well.  Since we named the `y_est` variable it is also tracked in every sampling step.  This makes plotting the results easier since we don't need to evaluate the model for every `b0` and `b1` value in the sample.\n",
    "\n",
    "Lets take a look at the covariance between the fitted intercept and slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fig = seaborn.jointplot(\n",
    "    x=traces_ols['intercept'],\n",
    "    y=traces_ols['slope'],\n",
    "    kind='hex',\n",
    "    height=8,\n",
    "    \n",
    ")\n",
    "fig.set_axis_labels('Centered Intercept', 'Centered Slope')\n",
    "seaborn.despine(top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the centered parameter space these two values are not correlated (as expected), let transform the fit back to the original data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def un_center(b0_prime, b1_prime, x_mean, x_std, y_mean, y_std):\n",
    "    b0 = (b0_prime * y_std) - (b1_prime * x_mean * y_std/x_std) + y_mean\n",
    "    x = x_mean - (x_std / b1_prime) * ((y_mean / y_std) + b0_prime)\n",
    "    b1 = -b0 / x\n",
    "    return b0, b1\n",
    "\n",
    "b0, b1 = un_center(traces_ols['intercept'], traces_ols['slope'], x_mean, x_std, y_mean, y_std)\n",
    "\n",
    "fig = seaborn.jointplot(\n",
    "    x=b0,\n",
    "    y=b1,\n",
    "    kind='hex',\n",
    "    height=8,\n",
    "    \n",
    ")\n",
    "fig.set_axis_labels('Intercept', 'Slope')\n",
    "seaborn.despine(top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a large correlation in the fit parameters.  If we tried to run the MCMC sampler in this data space it would have issues.\n",
    "\n",
    "### Plotting the best fit\n",
    "Finally we can plot the best fit on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# uncenter and scale\n",
    "y_est = (traces_ols['$y_{est}$'] * y_std) + y_mean\n",
    "\n",
    "# plot original data\n",
    "plt.figure(2, figsize=(12, 8))\n",
    "plt.errorbar(\n",
    "    data.x,\n",
    "    data.y,\n",
    "    data.sy,\n",
    "    ls='None',\n",
    "    mfc='k',\n",
    "    mec='k',\n",
    "    ms=5,\n",
    "    marker='s',\n",
    "    ecolor='k'\n",
    ")\n",
    "\n",
    "# find 2-sigma and meadian of best fit lines\n",
    "y_est_minus_2_sigma, y_est_median, y_est_plus_2_sigma = np.percentile(\n",
    "    y_est[:, idx],\n",
    "    [2.5, 50, 97.5],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# plot the mean of all best fit lines\n",
    "plt.plot(\n",
    "    data.x[idx],\n",
    "    y_est_median,\n",
    "    color='C3',\n",
    "    lw=3,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# plot 2-sigma best fit region\n",
    "plt.fill_between(\n",
    "    data.x[idx],\n",
    "    y_est_minus_2_sigma,\n",
    "    y_est_plus_2_sigma,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1\n",
    ")\n",
    "\n",
    "# plot a selection of best fit lines\n",
    "plt.plot(\n",
    "    data.x[idx],\n",
    "    y_est[::900].T[idx],\n",
    "    alpha=0.3,\n",
    "    color='C7'\n",
    ")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that the fitted line is being effected by several outliers in the data. Lets build a model that will filter out these outliers.\n",
    "\n",
    "## Mixture model\n",
    "\n",
    "To use `pymc3` we need to define the likelihood function we are trying to maximize.  The likelihood we need for this fit is shown in equation 13 of [Hogg 2010](http://arxiv.org/abs/1008.4686):\n",
    "\n",
    "$$\\mathscr{L} \\propto \\prod_{i=1}^{N}{\\left[\\frac{1}{\\sqrt{2 \\pi \\sigma_{yi}^{2}}} \\exp{\\left(-\\frac{[y_{i} - m x_{i} - b]^2}{2 \\sigma_{yi}^{2}}\\right)}\\right]^{[1-q_{i}]} \\times \\left[\\frac{1}{\\sqrt{2 \\pi [V_{b}+\\sigma_{yi}^{2}]}} \\exp{\\left( -\\frac{[y_{i} - Y_{b}]^2}{2[V_{b}+\\sigma_{yi}^{2}]} \\right)} \\right]^{q_{i}}}$$\n",
    "\n",
    "$$\\{q_{i}\\} \\sim \\text{Bernoulli}(P_b) $$\n",
    "\n",
    "where $x_{i}, y_{i}, \\sigma_{yi}$ are the data from the `.csv` file, $m, b$ are the slope and intercept of the line we are fitting, $q_{i}$ is $1$ if a point is an outlier and $0$ otherwise.  The set of these flags follow a Bernoulli distribution with a probability of being an outlier of $P_b$.  $Y_{b}, V_{b}$ are the parameters of the distribution the outliers are draw from.  See section 3 of Hogg's paper for a full derivation.\n",
    "\n",
    "Since we will be using a Bayesian approach to this fitting, we need to define priors for our fit parameters $\\theta = \\left[ m, b, P_b, \\{q_{i}\\}, Y_b, V_b \\right]$.  We will use weakly informative priors in all the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mdl_signoise:\n",
    "    #  state input data as Theano shared vars\n",
    "    tsv_x = pm.Data('tsv_x', x_center)\n",
    "    tsv_y = pm.Data('tsv_y', y_center)\n",
    "    tsv_sy = pm.Data('tsv_sy', sy_center)\n",
    "    # Define weakly informative Normal priros (Ridge regression)\n",
    "    b0 = pm.Normal('intercept', mu=0, sd=100)\n",
    "    b1 = pm.Normal('slope', mu=0, sd=100)\n",
    "    # Define y_est as a Deterministic variable so we can plot it later\n",
    "    y_est = pm.Deterministic(r'$y_{est}$', b0 + b1 * tsv_x)\n",
    "    # Define weakly informative priors for the mean and variance of the outliers\n",
    "    Yb = pm.Normal(r'$Y_b$', mu=0, sd=10)\n",
    "    Vb = pm.InverseGamma(r'$V_b$', 2, 5)\n",
    "    # crate in/outlier distributions to get logL evaluated on observed data\n",
    "    logL_in = pm.Normal.dist(mu=y_est, sigma=tsv_sy).logp(tsv_y)\n",
    "    sigma_out = pm.math.sqrt(Vb + tsv_sy**2)\n",
    "    logL_out = pm.Normal.dist(mu=Yb, sigma=sigma_out).logp(tsv_y)\n",
    "    # Define Bernoulli inlier / outlier probability (Pb) with uniform prior\n",
    "    Pb = pm.Uniform(\n",
    "        r'$P_b$',\n",
    "        lower=0.0,\n",
    "        upper=1.0,\n",
    "        testval=0.5\n",
    "    )\n",
    "    qi = pm.Bernoulli(\n",
    "        r'$q_i$',\n",
    "        p=Pb,\n",
    "        shape=tsv_x.eval().shape[0],\n",
    "        testval=np.random.rand(tsv_x.eval().shape[0]) < 0.5\n",
    "    )\n",
    "    # User custom likelihood\n",
    "    potential = pm.Potential(\n",
    "        'likelihood',\n",
    "        ((1 - qi) * logL_in).sum() + (qi * logL_out).sum()\n",
    "    )\n",
    "    \n",
    "display(mdl_signoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_signoise:\n",
    "    traces_signoise = pm.sample(\n",
    "        3000,\n",
    "        tune=2000,\n",
    "        chains=3,\n",
    "        cores=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see any warning messages about divergent chains it typically means one or more of your priors are not set to reasonable distributions, or some of your parameters have high covariance and should be rescaled.  In the above example if the prior on $V_b$ is set to be `Uniform` on the the $\\log(V_b)$ (as suggested by Hogg 2010) it will lead to divergent chains, but changing this to be an `InverseGamma` function clears up the issues.  Even with this change we also need to increase `target_accept` from its default of 0.8 to 0.999 to avoid divergent chains.\n",
    "\n",
    "### Note about priors\n",
    "Often times the priors you use will effect how fast the sampler will run.  If you are getting a small number of draws/s try changing all your priors to `Normal` distributions to see if it runs any faster (`Uniform` priors can be very slow if they cover a large range).\n",
    "\n",
    "See https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations for other tips about picking priors.\n",
    "\n",
    "### Check for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_signoise:\n",
    "    display(pm.summary(\n",
    "        traces_signoise,\n",
    "        var_names=[\n",
    "            'intercept',\n",
    "            'slope',\n",
    "            '$V_b$',\n",
    "            '$Y_b$',\n",
    "            '$P_b$'\n",
    "        ]\n",
    "    ))\n",
    "    pm.traceplot(\n",
    "        traces_signoise,\n",
    "        var_names=[\n",
    "            'intercept',\n",
    "            'slope',\n",
    "            '$V_b$',\n",
    "            '$Y_b$',\n",
    "            '$P_b$'\n",
    "        ]\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can see the `Rhat` values are all close to 1 and all the chains are well mixed.\n",
    "\n",
    "Lets take a look at a corner plot of these fit variables after converting the slope and intercept back into the original data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "df = pm.trace_to_dataframe(traces_signoise)[\n",
    "    [\n",
    "        'intercept',\n",
    "        'slope',\n",
    "        '$V_b$',\n",
    "        '$Y_b$',\n",
    "        '$P_b$'\n",
    "    ]\n",
    "]\n",
    "\n",
    "b0, b1 = un_center(df['intercept'], df['slope'], x_mean, x_std, y_mean, y_std)\n",
    "\n",
    "df['intercept'] = b0\n",
    "df['slope'] = b1\n",
    "\n",
    "def hide_current_axis(*args, **kwargs):\n",
    "    plt.gca().set_visible(False)\n",
    "\n",
    "\n",
    "g = seaborn.pairplot(\n",
    "    df,\n",
    "    diag_kind='kde',\n",
    "    plot_kws={'alpha': 0.3}\n",
    ")\n",
    "g.map_upper(hide_current_axis)\n",
    "seaborn.despine(top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we see a strong correlation between the slope and intercept.\n",
    "\n",
    "### Finding the outliers\n",
    "Unlike the OLS fit, this fit also has information about the probability of each point being an outlier ($q_i$).  For each step in the MC chain a `True` or `False` array was recored indicating what points belonged to the outlier distribution.  Averaging these across each step in the chin gives us the probability of each point being an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(8, 8))\n",
    "plt.plot(\n",
    "    traces_signoise['$q_i$'].mean(axis=0),\n",
    "    range(20),\n",
    "    'o'\n",
    ")\n",
    "plt.vlines([0, 1], -1, 20, ['C0', 'C3'], '--')\n",
    "ax=plt.gca()\n",
    "ax.set_yticks(range(20))\n",
    "ax.set_yticklabels(['[{0}]'.format(i) for i in data.ID])\n",
    "plt.xlabel('Probabiltiy of being and outler')\n",
    "plt.ylabel('Index of dasta point')\n",
    "plt.ylim(20, -1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that three points are clearly marked as being outliers 100% of the time.  All of the other points are classed as outliers less that 33% of the time.\n",
    "\n",
    "Lets take a closer look at the posterior distribution for the fraction of data points belonging to the outlier distribution ($P_b$).  Looking at the above plot we might expect this to peak at $3/20 = 0.15$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_signoise:\n",
    "    pm.plot_posterior(\n",
    "        traces_signoise,\n",
    "        var_names=[\n",
    "            '$P_b$'\n",
    "        ],\n",
    "        figsize=(12, 5),\n",
    "    #     text_size=16,\n",
    "        point_estimate='mode'\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly it peaks at 0.217, so we would expect 4 to 5 outliers instead of 3, so where does this number come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_signoise['$q_i$'].mean(axis=0).sum() / 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is closer to the peak of the posterior.  This is taking the sum of the outlier fraction for *all* points into account.  So overall there are \"5\" outliers but 2 of those are split among 17 data points.\n",
    "\n",
    "### Plotting the fits\n",
    "As before lets plot these fits on the original data points but this time we will highlight the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# uncenter and scale\n",
    "y_est = (traces_signoise['$y_{est}$'] * y_std) + y_mean\n",
    "\n",
    "# find the outliers\n",
    "prob_outlier = traces_signoise['$q_i$'].mean(axis=0)\n",
    "outliers = prob_outlier > 0.8\n",
    "\n",
    "plt.figure(4, figsize=(12, 8))\n",
    "# plot non-outliers\n",
    "plt.errorbar(\n",
    "    data.x[~outliers],\n",
    "    data.y[~outliers],\n",
    "    data.sy[~outliers],\n",
    "    ls='None',\n",
    "    mfc='k',\n",
    "    mec='k',\n",
    "    ms=5,\n",
    "    marker='s',\n",
    "    ecolor='k'\n",
    ")\n",
    "# plot outliers\n",
    "plt.errorbar(\n",
    "    data.x[outliers],\n",
    "    data.y[outliers],\n",
    "    data.sy[outliers],\n",
    "    ls='None',\n",
    "    mfc='C3',\n",
    "    mec='C3',\n",
    "    ms=5,\n",
    "    marker='s',\n",
    "    ecolor='C3'\n",
    ")\n",
    "\n",
    "y_est_minus_2_sigma, y_est_median, y_est_plus_2_sigma = np.percentile(\n",
    "    y_est[:, idx],\n",
    "    [2.5, 50, 97.5],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# plot the mean of all best fit lines\n",
    "plt.plot(\n",
    "    data.x[idx],\n",
    "    y_est_median,\n",
    "    color='C3',\n",
    "    lw=3,\n",
    "    zorder=3\n",
    ")\n",
    "# plot 2-sigma best fit region\n",
    "plt.fill_between(\n",
    "    data.x[idx],\n",
    "    y_est_minus_2_sigma,\n",
    "    y_est_plus_2_sigma,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1\n",
    ")\n",
    "# plot a selection of best fit lines\n",
    "plt.plot(\n",
    "    data.x[idx],\n",
    "    y_est[::500].T[idx],\n",
    "    alpha=0.2,\n",
    "    color='C7'\n",
    ")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better!  Our best fit line goes through all the data points without being confused by the outliers.\n",
    "\n",
    "### What if you don't care about $q_i$?\n",
    "\n",
    "If you don't care about the $q_i$ value for each data point you they can be marginalized over.  Doing this results in a normal mixture model, and pymc3 has this built in.  The plus side to using this kind of model is every variable is continuous ($q_i$ was discrete) making it easier to sample from and faster to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mixture:\n",
    "    w = pm.Dirichlet('w', np.array([0.8, 0.2]))\n",
    "    b0 = pm.Normal('intercept', mu=0, sd=100, testval=0)\n",
    "    b1 = pm.Normal('slope', mu=0, sd=100, testval=1.2)\n",
    "    Yb = pm.Normal(r'$Y_b$', mu=0, sd=10, testval=0)\n",
    "    Vb = pm.InverseGamma(r'$V_b$', 2, 5, testval=2.5)\n",
    "    y_est = pm.Deterministic(\n",
    "        r'$y_{est}$',\n",
    "        tt.stack(\n",
    "            [\n",
    "                b0 + b1 * x_center,\n",
    "                np.ones_like(x_center) * Yb\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "    sigma = pm.Deterministic(\n",
    "        r'$\\sigma$',\n",
    "        tt.stack(\n",
    "            [\n",
    "                sy_center,\n",
    "                np.sqrt(np.array(sy_center)**2 + Vb)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "    likelihood = pm.NormalMixture(\n",
    "        'likelihood',\n",
    "        w,\n",
    "        y_est,\n",
    "        sd=sigma,\n",
    "        observed=y_center\n",
    "    )\n",
    "\n",
    "display(mixture)\n",
    "display(pm.model_to_graphviz(mixture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w$ is a two element array that is equivalent to $[1-P_b, P_b]$.  These are the coefficients used to say how much of the likelihood comes from the inlier and outlier distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mixture:\n",
    "    traces_mixture = pm.sample(\n",
    "        1000,\n",
    "        tune=1000,\n",
    "        target_accept=0.9,\n",
    "        chains=3,\n",
    "        cores=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mixture:\n",
    "    display(pm.summary(\n",
    "        traces_mixture,\n",
    "        var_names=[\n",
    "            'intercept',\n",
    "            'slope',\n",
    "            '$V_b$',\n",
    "            '$Y_b$',\n",
    "            'w'\n",
    "        ]\n",
    "    ))\n",
    "    pm.traceplot(\n",
    "        traces_mixture,\n",
    "        var_names=[\n",
    "            'intercept',\n",
    "            'slope',\n",
    "            '$V_b$',\n",
    "            '$Y_b$',\n",
    "            'w'\n",
    "        ]\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good.  If you compare the values in the summary table to the values found in the previous fit they are in agreement for all values.\n",
    "\n",
    "## How to deal with errors in the $x$ direction\n",
    "\n",
    "Up until now we have only taken into account the errorbars in the $y$ direction. If you also wanted to account for the errors in the $x$ direction (assuming there are no covariances) the OLS model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mdl_ols_sx:\n",
    "    b0 = pm.Normal('intercept', mu=0, sd=100)\n",
    "    b1 = pm.Normal('slope', mu=0, sd=100)\n",
    "    x_est = pm.Normal('$x_{est}$', mu=0, sd=50, shape=len(x_center))\n",
    "    likelihood_x = pm.Normal('likelihood$_x$', mu=x_est, sd=sx_center, observed=x_center)\n",
    "    y_est = pm.Deterministic('$y_{est}$', b0 + b1 * x_est)\n",
    "    likelihood_y = pm.Normal('likelihood$_y$', mu=y_est, sd=sy_center, observed=y_center)\n",
    "\n",
    "display(mdl_ols_sx)\n",
    "display(pm.model_to_graphviz(mdl_ols_sx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much like the model before, except this time we assume a `Normal` prior on the $x$ positions and add in a second likelihood using the observed $x$ data.\n",
    "\n",
    "### Note\n",
    "This is a case where placing a `Uniform` prior on $x_{est}$ will cause it to take much longer to run (over 20 mins as opposed to 20 secs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_ols_sx:\n",
    "    traces_ols_2 = pm.sample(3000, tune=2000, chains=3, cores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with mdl_ols_sx:\n",
    "    display(pm.summary(traces_ols_2, var_names=['intercept', 'slope']))\n",
    "    pm.traceplot(traces_ols_2, var_names=['intercept', 'slope']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Care should be taken when plotting these results as each $y_{est}$ has been calculated using slightly different $x_{est}$ values, so they can't be averaged as nicely as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# get y_est evaluated at all the same x positinos\n",
    "x_eval = np.linspace(-3, 3, 200)\n",
    "y_eval = traces_ols_2['slope'].reshape(-1, 1) * x_eval + traces_ols_2['intercept'].reshape(-1, 1)\n",
    "\n",
    "# uncenter data\n",
    "x_eval = (x_eval * x_std) + x_mean\n",
    "y_eval = (y_eval * y_std) + y_mean\n",
    "\n",
    "# get 2-sigma region and median\n",
    "y_est_minus_2_sigma, y_est_median, y_est_plus_2_sigma = np.percentile(\n",
    "    y_eval,\n",
    "    [2.5, 50, 97.5],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "plt.figure(5, figsize=(12, 8))\n",
    "plt.errorbar(\n",
    "    data.x,\n",
    "    data.y,\n",
    "    data.sy,\n",
    "    data.sx,\n",
    "    ls='None',\n",
    "    mfc='k',\n",
    "    mec='k',\n",
    "    ms=5,\n",
    "    marker='s',\n",
    "    ecolor='k'\n",
    ")\n",
    "\n",
    "# plot the mean of all best fit lines\n",
    "plt.plot(\n",
    "    x_eval,\n",
    "    y_est_median,\n",
    "    color='C3',\n",
    "    lw=3,\n",
    "    zorder=3\n",
    ")\n",
    "# plot 2-sigma best fit region\n",
    "plt.fill_between(\n",
    "    x_eval,\n",
    "    y_est_minus_2_sigma,\n",
    "    y_est_plus_2_sigma,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1\n",
    ")\n",
    "# plot a selection of best fit lines\n",
    "plt.plot(\n",
    "    x_eval,\n",
    "    y_eval[::200].T,\n",
    "    alpha=0.2,\n",
    "    color='C7'\n",
    ")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(0, 700);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar setup can be used in either of the mixture models used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
