{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "At times you don't care about the underlying model for your data points and just want a model that describes the data.  One such fitting technique is know as Gaussian process regression (also know as kriging).  This kind of regression assumes all the data points are drawn from a common covariance function.  This function is used to generate an (infinite) set of functions and only keeps the ones that pass through the observed data.\n",
    "\n",
    "## Packages being used\n",
    "+ `pymc3`: has a Gaussian process regression function\n",
    "\n",
    "## Relevant documentation\n",
    "+ `pymc3`: https://docs.pymc.io/notebooks/GP-MeansAndCovs.html, https://docs.pymc.io/notebooks/GP-Marginal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from scipy import interpolate\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "import mpl_style\n",
    "%matplotlib inline\n",
    "plt.style.use(mpl_style.style1)\n",
    "seaborn.axes_style(mpl_style.style1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The squared exponential covariance (or Radial-basis function or Exponential Quadratic)\n",
    "As an example we will use the squared exponential covariance function:\n",
    "$$ \\operatorname{Cov}{(x_1, x_2; h)} = \\exp{\\left( \\frac{-(x_1 - x_2)^2}{2h^2} \\right)} $$\n",
    "Lets using this function to draw some _unconstrained_ functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1\n",
    "cov = pm.gp.cov.ExpQuad(1, h)\n",
    "\n",
    "x = np.linspace(1, 10, 500)[:, None]\n",
    "K = cov(x).eval()\n",
    "\n",
    "plt.figure(1, figsize=(18, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x, pm.MvNormal.dist(mu=np.zeros(K.shape[0]), cov=K).random(size=6).T)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(K, interpolation='none', origin='upper', extent=[0, 10, 10, 0])\n",
    "plt.colorbar()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrain the model\n",
    "\n",
    "Assume we have some data points, we can use Gaussian process regression to only pick the models that pass through those points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1, 3, 5, 6, 7, 8])\n",
    "y1 = x1 * np.sin(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the PYMC model\n",
    "We will define priors for the length scale `h` and the leading scaling coefficient `c`.  We will assume there is a small level of equal but unknown noise associated with each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x1[:, None]\n",
    "with pm.Model() as model:\n",
    "    h = pm.Gamma(\"h\", alpha=2, beta=1)\n",
    "    c = pm.HalfCauchy(\"c\", beta=5)\n",
    "    cov = c**2 * pm.gp.cov.ExpQuad(1, ls=h)\n",
    "    gp = pm.gp.Marginal(cov_func=cov)\n",
    "    noise = pm.HalfCauchy(\"noise\", beta=0.1)\n",
    "    y_fit = gp.marginal_likelihood(\"y_{fit}\", X=X, y=y1, noise=noise)\n",
    "\n",
    "display(model)\n",
    "display(pm.model_to_graphviz(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum of the likelihood using the `find_MAP` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    mp = pm.find_MAP()\n",
    "\n",
    "display('Best fit kernel: {0:.2f}**2 * ExpQuad(ls={1:.2f})'.format(mp['c'], mp['h']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fit to interpolate to new `X` values\n",
    "This `MAP` fit can be used to interpolate and extrapolate to a new grid of points.  PYMC offers the `predict` method to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = 500\n",
    "X_new = np.linspace(0, 10, n_new)\n",
    "\n",
    "mu, var = gp.predict(X_new[:,None], point=mp, diag=True)\n",
    "sd = np.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(10, 8))\n",
    "plt.plot(x1, y1, 'ok', label='observed')\n",
    "\n",
    "plt.plot(\n",
    "    X_new,\n",
    "    X_new * np.sin(X_new),\n",
    "    '--',\n",
    "    color='C1',\n",
    "    label='True'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    X_new.flatten(),\n",
    "    mu,\n",
    "    color='C0',\n",
    "    lw=3,\n",
    "    zorder=3,\n",
    "    label='prediction'\n",
    ")\n",
    "\n",
    "# plot 95% best fit region\n",
    "plt.fill_between(\n",
    "    X_new.flatten(),\n",
    "    mu - 1.96*sd,\n",
    "    mu + 1.96*sd,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1,\n",
    "    label='95% confidence interval'\n",
    ")\n",
    "\n",
    "# labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-6, 12)\n",
    "plt.legend(loc='upper left', ncol=2)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the data\n",
    "Sometimes you want to know the values of the covariance function and draw samples from the posterior distribution.  We can do easily do this within PYMC with the `sample` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, target_accept=0.99)\n",
    "    display(pm.summary(trace))\n",
    "    pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pm.trace_to_dataframe(trace)\n",
    "\n",
    "def hide_current_axis(*args, **kwargs):\n",
    "    plt.gca().set_visible(False)\n",
    "\n",
    "g = seaborn.pairplot(\n",
    "    df,\n",
    "    diag_kind='kde',\n",
    "    plot_kws={'alpha': 0.3}\n",
    ")\n",
    "g.map_upper(hide_current_axis)\n",
    "seaborn.despine(top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the trade off between the noise level and the `c` parameter.  When `c` becomes small the noise becomes large, i.e. it models all the points as coming from a flat line with high noise.\n",
    "\n",
    "Now that we have sampled from the distribution we can interpolate/extrapolate the fitted function.  To do this we have to pass the new `X` values into the model as a `conditional`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    f_new = gp.conditional('f_new', Xnew=X_new[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can draw samples of these conditional fits from the posterior using `sample_posterior_predictive`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    ppc = pm.sample_posterior_predictive(trace, samples=200, vars=[f_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can make a plot of these samples using the `plot_gp_dist` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(6, figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.plot(x1, y1, 'ok', label='observed')\n",
    "\n",
    "pm.gp.util.plot_gp_dist(\n",
    "    ax,\n",
    "    ppc['f_new'],\n",
    "    X_new,\n",
    "    plot_samples=False,\n",
    "    palette=\"Blues\",\n",
    "    fill_alpha=1\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    X_new,\n",
    "    X_new * np.sin(X_new),\n",
    "    '--',\n",
    "    color='C1',\n",
    "    label='True'\n",
    ")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-10, 12)\n",
    "plt.legend(loc='upper left', ncol=2)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is slightly different than previous plot, this is due the the nature of the fitting process used.  In the first case we used the maximum of the likelihood distribution as a point estimate of the best fit, and in the second case we sampled directly from the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy data\n",
    "Let's add some noise to the data.  We will assume each data point has independent errorbars.  These values can be passed directly into the `marginal_likelihood` function instead of the prior we were using before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = 0.5 + np.random.random(y1.shape)\n",
    "y_noise = np.random.normal(0, dy)\n",
    "y2 = y1 + y_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_noise:\n",
    "    h = pm.Gamma(\"h\", alpha=2, beta=1)\n",
    "    c = pm.HalfCauchy(\"c\", beta=5)\n",
    "    cov = c**2 * pm.gp.cov.ExpQuad(1, ls=h)\n",
    "    gp = pm.gp.Marginal(cov_func=cov)\n",
    "    y_fit = gp.marginal_likelihood(\"y_{fit}\", X=X, y=y2, noise=dy)\n",
    "\n",
    "display(model_noise)\n",
    "display(pm.model_to_graphviz(model_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_noise:\n",
    "    mp_noise = pm.find_MAP()\n",
    "\n",
    "display('Best fit kernel: {0:.2f}**2 * ExpQuad(ls={1:.2f})'.format(mp_noise['c'], mp_noise['h']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results\n",
    "As before we can interpolate and extrapolate to new points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_noise, var_noise = gp.predict(X_new[:,None], point=mp_noise, diag=True)\n",
    "sd_noise = np.sqrt(var_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(10, 8))\n",
    "plt.errorbar(x1, y2, yerr=1.96*dy, fmt='ok', label='observed')\n",
    "\n",
    "plt.plot(\n",
    "    X_new,\n",
    "    X_new * np.sin(X_new),\n",
    "    '--',\n",
    "    color='C1',\n",
    "    label='True'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    X_new.flatten(),\n",
    "    mu_noise,\n",
    "    color='C0',\n",
    "    lw=3,\n",
    "    zorder=3,\n",
    "    label='prediction'\n",
    ")\n",
    "\n",
    "# plot 95% best fit region\n",
    "plt.fill_between(\n",
    "    X_new.flatten(),\n",
    "    mu_noise - 1.96*sd_noise,\n",
    "    mu_noise + 1.96*sd_noise,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1,\n",
    "    label='95% confidence interval'\n",
    ")\n",
    "\n",
    "# labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-10, 12)\n",
    "plt.legend(loc='upper left', ncol=2)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Cubic Spline\n",
    "So far we have been using the `ExpQuad` kernel, but it there are others that can be used.  You may have noticed that this method of fitting provides smooth curves that pass through the data points very similar to how a spline fit does.  As it turns out, a spline fit is just a special case of a Gaussian process fit.  To recreate a cubic spline we can use the following kernel:\n",
    "$$ \\operatorname{Cov}(x_1, x_2) = 1 + \\lvert x_1 - x_2 \\rvert \\frac{\\min(x_1, x_2)^2}{2} + \\frac{\\min(x_1, x_2)^3}{3}$$\n",
    "\n",
    "Under the condition that all values of $x_1$ and $x_2$ are between the values of $0$ and $1$.  This normalization ensure that the covariance matrix is positive definite.  So unlike other kernels we will needed to know the range we plan to extrapolate onto before doing our fit.\n",
    "\n",
    "This kernel is not built into `pymc3` so we will have to write a custom kernel for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubicSpline(pm.gp.cov.Covariance):\n",
    "    def __init__(self, dim, x_min=0, x_max=1):\n",
    "        super(CubicSpline, self).__init__(1, None)\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def norm(self, X):\n",
    "        d = self.x_max - self.x_min\n",
    "        return (X - self.x_min) / d\n",
    "\n",
    "    def diag(self, X):\n",
    "        X, _ = self._slice(X, Xs=None)\n",
    "        X = self.norm(X)\n",
    "        Xt = tt.flatten(X)\n",
    "        return 1 + (Xt**3) / 3\n",
    "\n",
    "    def full(self, X, Xs=None):\n",
    "        X, Xs = self._slice(X, Xs)\n",
    "        if Xs is None:\n",
    "            Xs = X\n",
    "        X = self.norm(X)\n",
    "        Xs = self.norm(Xs)\n",
    "        d = tt.abs_(X - tt.transpose(Xs))\n",
    "        v = tt.minimum(X, tt.transpose(Xs))\n",
    "        k = 1 + (0.5 * d * v**2) + ((v**3) / 3)\n",
    "        return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit for this kernel's coefficient.  The prior for the noise is pushed to lower values to ensure the fit does not treat the data as \"noise only.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_cube:\n",
    "    tau = pm.HalfCauchy('tau', beta=5)\n",
    "    # we know we will be interpolating onto points between 0 and 10, so initalize the spline kernel with these values\n",
    "    cov = CubicSpline(1, x_min=0, x_max=10) * tau**2\n",
    "    gp = pm.gp.Marginal(cov_func=cov)\n",
    "    noise = pm.HalfCauchy('noise', beta=0.1)\n",
    "    y_fit = gp.marginal_likelihood(\"y_{fit}\", X=x1[:, None], y=y1, noise=noise)\n",
    "\n",
    "display(model_cube)\n",
    "display(pm.model_to_graphviz(model_cube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_cube:\n",
    "    mp_cube = pm.find_MAP()\n",
    "\n",
    "display('{0:.2f}**2 * CubicSpline'.format(mp_cube['tau']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_cube, var_cube = gp.predict(X_new[:,None], point=mp_cube, diag=True)\n",
    "sd_cube = np.sqrt(var_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also fit a cubic spline to the data and compare it to the Gaussian process fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tck = interpolate.splrep(x1, y1, k=3)\n",
    "y_new = interpolate.splev(X_new, tck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(10, 8))\n",
    "plt.plot(x1, y1, 'ok', label='observed')\n",
    "\n",
    "plt.plot(\n",
    "    X_new,\n",
    "    X_new * np.sin(X_new),\n",
    "    '--',\n",
    "    color='C1',\n",
    "    label='True'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    X_new.flatten(),\n",
    "    mu_cube,\n",
    "    color='C0',\n",
    "    lw=3,\n",
    "    zorder=3,\n",
    "    label='prediction'\n",
    ")\n",
    "\n",
    "# plot 95% best fit region\n",
    "plt.fill_between(\n",
    "    X_new.flatten(),\n",
    "    mu_cube - 1.96*sd_cube,\n",
    "    mu_cube + 1.96*sd_cube,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1,\n",
    "    label='95% confidence interval'\n",
    ")\n",
    "\n",
    "plt.plot(X_new, y_new, color='C3', label='Cubic spline', lw=4)\n",
    "\n",
    "# labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-10, 15)\n",
    "plt.legend(loc='upper left', ncol=2)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that aside from the end points the Gaussian process the the spline give the same result.  Additionally we now have an errorbar estimate for a spline fit!\n",
    "\n",
    "## Adding noise to the spline fit\n",
    "Now that we have this working we can include measurement errors on each of the data points like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_cube_err:\n",
    "    tau = pm.HalfCauchy('tau', beta=5)\n",
    "    cov = CubicSpline(1, x_min=0, x_max=10) * tau**2\n",
    "    gp = pm.gp.Marginal(cov_func=cov)\n",
    "    y_fit = gp.marginal_likelihood(\"y_{fit}\", X=x1[:, None], y=y2, noise=dy)\n",
    "\n",
    "display(model_cube_err)\n",
    "display(pm.model_to_graphviz(model_cube_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_cube_err:\n",
    "    mp_cube_err = pm.find_MAP()\n",
    "\n",
    "display('{0:.2f}**2 * CubicSpline'.format(mp_cube_err['tau']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_cube_err, var_cube_err = gp.predict(X_new[:,None], point=mp_cube_err, diag=True)\n",
    "sd_cube_err = np.sqrt(var_cube_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tck_noise = interpolate.splrep(x1, y2, k=3)\n",
    "y_new_noise = interpolate.splev(X_new, tck_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5, figsize=(10, 8))\n",
    "plt.errorbar(x1, y2, yerr=1.96*dy, fmt='ok', label='observed')\n",
    "\n",
    "plt.plot(\n",
    "    X_new,\n",
    "    X_new * np.sin(X_new),\n",
    "    '--',\n",
    "    color='C1',\n",
    "    label='True'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    X_new.flatten(),\n",
    "    mu_cube_err,\n",
    "    color='C0',\n",
    "    lw=3,\n",
    "    zorder=3,\n",
    "    label='prediction'\n",
    ")\n",
    "\n",
    "# plot 95% best fit region\n",
    "plt.fill_between(\n",
    "    X_new.flatten(),\n",
    "    mu_cube_err - 1.96*sd_cube_err,\n",
    "    mu_cube_err + 1.96*sd_cube_err,\n",
    "    color='C0',\n",
    "    alpha=0.3,\n",
    "    zorder=1,\n",
    "    label='95% confidence interval'\n",
    ")\n",
    "\n",
    "plt.plot(X_new, y_new_noise, color='C3', label='Cubic spline', lw=4)\n",
    "\n",
    "# labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-10, 15)\n",
    "plt.legend(loc='upper left', ncol=2)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other notes\n",
    "+ There are many covariance kernels you can pick;\n",
    "    + `Constant`: a constant value that can be multiplied or added to any of the other kernels\n",
    "    + `WhiteNoise`: a white noise kernel\n",
    "    + `ExpQud`: exponentiated quadratic, smooth kernel parameterized by a length-scale\n",
    "    + `RatQuad`: rational quadratic, a (infinite sum) mixture of different `ExpQud`'s each with different length-scales\n",
    "    + `Exponential`: Similar to `ExpQud` but without the square in the exponent.\n",
    "    + `Marten52`: Marten 5/2 non-smooth generalization of `RBF`, parameterized by length-scale and smoothness\n",
    "    + `Marten32`: Marten 3/2 non-smooth generalization of `RBF`, parameterized by length-scale and smoothness\n",
    "    + `Cosine`: periodic kernel built with `cos`\n",
    "    + `Linear`: a non-stationary kernel that can be used to fit a line\n",
    "    + `Polynomial`: a non-stationary kernel commonly polynomial like fit\n",
    "    + `Periodic`: periodic function kernel, parameterized by a length-scale and periodicity\n",
    "\n",
    "+ There are also three mean functions to choose from:\n",
    "    + `Zero`: The mean is all zeros (this is the default)\n",
    "    + `Constant`: The mean is a constant value (i.e. a global y-offset)\n",
    "    + `Linear`: The mean is a linear function (i.e. a polynomial)\n",
    "\n",
    "+ See https://docs.pymc.io/notebooks/GP-MeansAndCovs.html for examples of each kernel and mean function\n",
    "+ All `X` positions must be unique\n",
    "+ The computational complexity is $O(N^3)$ where $N$ is the number of data point.  If you have a large number of data points you can use PYMC3's variational inference methods (https://docs.pymc.io/api/inference.html#variational) that replace the (complex) posterior with simpler approximations that are faster to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
